{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99201bae",
   "metadata": {},
   "source": [
    "# Extended Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e1fbd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "             .container { width:900px !important; }\n",
       "             .figure {display:table; margin:auto;}\n",
       "             .img {float: left; margin: 10px;}\n",
       "             .caption {display:table-caption;caption-side:bottom;text-align:justify;color: rgba(0,0,0,0.6);}\n",
       "             td { background: white;}\n",
       "            .table_caption {\n",
       "                font-size:14px;\n",
       "                float: left;\n",
       "                text-align: justify;\n",
       "                color: rgba(0,0,0,0.6);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gtn\n",
    "import math\n",
    "import numpy as np\n",
    "import nb_utils\n",
    "nb_utils.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c2cca",
   "metadata": {},
   "source": [
    "## Counting $n$-grams\n",
    "\n",
    "In this example, we'll use graph operations to count the number of $n$-grams in a string. \n",
    "\n",
    "Suppose we have a string $aaabaa$ and we want to know the frequency of each bigram. In this case the bigrams contained in the string are $aa$, $ab$, and $ba$ with frequencies of $3$, $1$, and $1$ respectively. In the general case we are given an input string $x$ and an $n$-gram, $y$, and the goal is to count the number of occurrences of $y$ in $x$.\n",
    "\n",
    "For a given $n$-gram, the first step is to construct the graph which matches that $n$-gram at any location in the string. We want to construct the graph equivalent of the regular expression $.*y.*$ where $.*$ indicates zero of more occurrences of any token in the token set. \n",
    "\n",
    "Suppose we want to count the number of ocurrences of the bigram $aa$ in $aaabaa$. For the bigram $aa$, and the token set alphabet $\\{a, b, c\\}$, the $n$-gram matching graph is shown below.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/bigram_aa.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "      A hierarchy of classes graph.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "We can encode the string $aaabaa$ as a simple chain graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab21d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = {0: \"a\", 1: \"b\", 2: \"c\"}\n",
    "\n",
    "# Encode the string \"aaabaa\" as integer ids:\n",
    "x = [0, 0, 0, 1, 0, 0]  \n",
    "g = gtn.Graph()\n",
    "g.add_node(start=True)\n",
    "for i, l in enumerate(x):\n",
    "    g.add_node(accept=(i + 1==len(x)))\n",
    "    g.add_arc(src_node=i, dst_node=i + 1, label=l)\n",
    "gtn.draw(g, \"figures/nb/ngram_string.svg\", isymbols=symbols)\n",
    "\n",
    "bigram = gtn.Graph()\n",
    "bigram.add_node(start=True)\n",
    "bigram.add_node()\n",
    "bigram.add_node(accept=True)\n",
    "bigram.add_arc(src_node=0, dst_node=1, label=0)\n",
    "bigram.add_arc(src_node=1, dst_node=2, label=0)\n",
    "for l in range(len(symbols)):\n",
    "    bigram.add_arc(src_node=0, dst_node=0, label=l)\n",
    "    bigram.add_arc(src_node=2, dst_node=2, label=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228718b",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/ngram_string.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "      A hierarchy of classes graph.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "We then compute the intersection of the graph representing the string and the graph representing the bigram. The number of paths in this graph represents the number of occurrences of the bigram in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b849b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_paths = gtn.intersect(g, bigram)\n",
    "gtn.draw(bigram_paths, \"figures/nb/bigram_paths.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b245ed2",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/bigram_paths.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "      A hierarchy of classes graph.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Since each path has a weight of $0$, we can count the number of unique paths in the intersected graph by using the forward score. Assume the intersected graph has $p$ paths. The forward score of the graph is $s = \\log \\sum_{i=1}^p e^0 = \\log p$. So the total number of paths is $p = e^s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f37a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of occurrences of 'ab' in 'aaabaa' is 3\n"
     ]
    }
   ],
   "source": [
    "s = gtn.forward_score(bigram_paths)\n",
    "p = math.exp(s.item())\n",
    "print(f\"The number of occurrences of 'ab' in 'aaabaa' is {p:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb468b5",
   "metadata": {},
   "source": [
    "## Edit Distance\n",
    "\n",
    "In this example we'll use transducers to compute the Levenshtein edit distance between two sequences. The edit distance is a way to measure the similarity between to sequences by computing the minimum number of operations required to change one sequence into the other. The Levenshtein edit distance allows for insertion, deletion, and substitution operations. \n",
    "\n",
    "For example, consider the two strings \"saturday\" and \"sunday\". The edit distance between them is $3$. One way to minimally edit \"saturday\" to \"sunday\" is with two deletions (D) and a substitution (S) as below:\n",
    "```\n",
    "s a t u r d a y\n",
    "  D D   S\n",
    "s     u n d a y\n",
    "```\n",
    "\n",
    "We can compute the edit distance between two strings with the use transducers. The idea is to transduce the first string into the second according to the allowed operations encoded as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd4d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = gtn.Graph()\n",
    "edits.add_node(True)\n",
    "edits.add_node(accept=True)\n",
    "edits.add_node(accept=True)\n",
    "edits.add_node(accept=True)\n",
    "edits.add_node(accept=True)\n",
    "edits.add_arc(0, 1, gtn.epsilon, 0, 1)\n",
    "edits.add_arc(0, 1, gtn.epsilon, 1, 1)\n",
    "edits.add_arc(0, 2, 0, gtn.epsilon, 1)\n",
    "edits.add_arc(0, 2, 1, gtn.epsilon, 1)\n",
    "edits.add_arc(0, 3, 0, 1, 1)\n",
    "edits.add_arc(0, 3, 1, 0, 1)\n",
    "edits.add_arc(0, 4, 0, 0)\n",
    "edits.add_arc(0, 4, 1, 1)\n",
    "gtn.draw(edits, \"figures/nb/edits.svg\", isymbols=symbols, osymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c652c",
   "metadata": {},
   "source": [
    "We first construct an edits graph $E$ which encodes the allowed operations. An example of an edits graph assuming a token set of $\\{a, b\\}$ is shown below. The insertion of a token is represented by the arcs from state $0$ to state $1$ hand has a cost of $1$. The deletion of a token is represented by the arcs from state $0$ to state $2$ which also incur a cost of $1$. All possible substitutions are encoded in the arcs from $0$ to $3$ and again have a cost of $1$. We also have to encode the possibility of leaving a token unchanged. This is represented on the arcs from $0$ to $4$, and the cost is $0$.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/edits.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "      TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "We then take closure of the edits graph $E$ to represent the fact that we can make zero or more of any of the allowed edits. We then encode the first string $x$ in a graph $X$ and the second string $y$ in a graph $Y$. All possible ways of editing $x$ to $y$ can be computed by taking the composition:\n",
    "\n",
    "$$\n",
    "P = X \\circ E^* \\circ Y\n",
    "$$\n",
    "\n",
    "The graph $P$ represents the set of all possible unique ways we can edit the string $x$ into $y$. The score of a given path in $P$ is the associated cost. We can then find the edit distance by computing the path with the smallest score in $P$. For this, we use Viterbi algorithm with a $\\min$ instead of a $\\max$. Alternatively, we can use weights of $-1$ instead of $1$ in $E$ and use the Viterbi algorithm unchanged. The actual edits (*i.e.* the insertions, deletions, and substitutions) can be found by computing the Viterbi path.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Compute the edit distance between $x = abab$ and $y = aaabb$ using graphs.\n",
    "\n",
    "We first construct the edits graph $E^*$ and the graphs $X$ and $Y$ corresponding to the strings $x$ and $y$. Note that we are using a minimal representation of $E^*$, but the graph is equivalent to the closure of the edits graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a65fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = gtn.Graph()\n",
    "edits.add_node(start=True, accept=True)\n",
    "edits.add_arc(0, 0, gtn.epsilon, 0, -1)\n",
    "edits.add_arc(0, 0, gtn.epsilon, 1, -1)\n",
    "edits.add_arc(0, 0, 0, gtn.epsilon, -1)\n",
    "edits.add_arc(0, 0, 1, gtn.epsilon, -1)\n",
    "edits.add_arc(0, 0, 0, 1, -1)\n",
    "edits.add_arc(0, 0, 1, 0, -1)\n",
    "edits.add_arc(0, 0, 0, 0)\n",
    "edits.add_arc(0, 0, 1, 1)\n",
    "\n",
    "X = gtn.Graph()\n",
    "X.add_node(start=True)\n",
    "X.add_node()\n",
    "X.add_node()\n",
    "X.add_node()\n",
    "X.add_node(accept=True)\n",
    "X.add_arc(0, 1, 0)\n",
    "X.add_arc(1, 2, 1)\n",
    "X.add_arc(2, 3, 0)\n",
    "X.add_arc(3, 4, 1)\n",
    "\n",
    "Y = gtn.Graph()\n",
    "Y.add_node(start=True)\n",
    "Y.add_node()\n",
    "Y.add_node()\n",
    "Y.add_node()\n",
    "Y.add_node()\n",
    "Y.add_node(accept=True)\n",
    "Y.add_arc(0, 1, 0)\n",
    "Y.add_arc(1, 2, 0)\n",
    "Y.add_arc(2, 3, 0)\n",
    "Y.add_arc(3, 4, 1)\n",
    "Y.add_arc(4, 5, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320c201",
   "metadata": {},
   "source": [
    "Note also that with the edits graph, $E^*$ we use scores of $-1$ so that we can use the Viterbi score which is the maximum scoring path to find the smallest edit distance. The next step is to compute $P = X \\circ E^* \\circ Y$.  The graph $P$ is shown below. Each path in $P$ represents a unique conversion of $X$ into $Y$ using insertion, deletion, and substitution operations. The negation of the score of the path is the number of such operations required.\n",
    "\n",
    "For example the path along the state sequence $0 \\rightarrow 1 \\rightarrow 4 \\rightarrow 9 \\rightarrow 16 \\rightarrow 25$ converts $x$ to $y$ with a substitution for the second letter and an insertion at the end:\n",
    "```\n",
    "x = a b a b\n",
    "y = a a a b b\n",
    "      S     I\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5636a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_paths = gtn.compose(X, gtn.compose(edits, Y))\n",
    "gtn.draw(edit_paths, \"figures/nb/edit_paths.svg\", isymbols=symbols, osymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12173dbf",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/edit_paths.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129e0d4",
   "metadata": {},
   "source": [
    "The Viterbi score and Viterbi path then yield the edit distance between $x$ and $y$ and one sequence of edit operations required to attain the edit distance. The Viterbi path for the example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9f2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The edit distance is the negation of the Viterbi score\n",
    "edit_distance = -gtn.viterbi_score(edit_paths).item()\n",
    "edit_path = gtn.viterbi_path(edit_paths)\n",
    "gtn.draw(edit_path, \"figures/nb/edit_path.svg\", isymbols=symbols, osymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e88d15",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/edit_path.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## <a name=\"ngrams\"></a>$n$-gram Language Model\n",
    "$\\DeclareMathOperator*{\\LSE}{\\textrm{LSE}}$\n",
    "In this example we'll encode an $n$-gram language model as an acceptor. We'll then use the acceptor to compute the language model probability of a given sequence.\n",
    "\n",
    "Let's start with a very simple example. Suppose we have the token set $\\{a, b, c, \\textrm{</s>}\\}$ and we want to construct a unigram langauge model. Note  $\\textrm{</s>}$ is the end of sentence token and is required for the language model to be a valid probability distribution. Given counts of occurrences for each token in the vocabulary, we can construct an acceptor to represent the unigram language model. Suppose we are given the log probabilities $0.5$, $0.2$, and $0.3$ for $a$, $b$, and $c$ respectively. The corresponding unigram graph is shown below. Note the edge weights are the log probabilities.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/unigram.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Now assume we are given the sequence $aa$ for which we would like to compute the probability. The probability under the language model is $\\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$. We can compute the log probability of $aa$ by intersecting its graph representation $X$ with the unigram graph $U$ and then computing the forward score:\n",
    "\n",
    "$$\n",
    "\\log p(aa) = \\LSE (X \\circ U)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2742490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unigram graph U:\n",
    "U = gtn.Graph()\n",
    "U.add_node(start=True, accept=True)\n",
    "#U.add_node(accept=True)\n",
    "U.add_arc(src_node=0, dst_node=0, label=0)\n",
    "U.add_arc(src_node=0, dst_node=0, label=1)\n",
    "U.add_arc(src_node=0, dst_node=0, label=2)\n",
    "U.set_weights([math.log(p) for p in [0.5, 0.2, 0.3]])\n",
    "\n",
    "# The graph representing the sequence \"aa\":\n",
    "X = gtn.Graph()\n",
    "X.add_node(start=True)\n",
    "X.add_node()\n",
    "X.add_node(accept=True)\n",
    "X.add_arc(src_node=0, dst_node=1, label=0)\n",
    "X.add_arc(src_node=1, dst_node=2, label=0)\n",
    "\n",
    "# Compute the unigram score of X:\n",
    "x_scored = gtn.intersect(X, U)\n",
    "x_prob = math.exp(gtn.forward_score(x_scored).item())\n",
    "gtn.draw(x_scored, \"figures/nb/unigram_aa_scored.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482fdfc",
   "metadata": {},
   "source": [
    "The graph below shows the intersection $X \\circ U$. The arc edges in the intersected graph contain the correct unigram scores, and the forward score gives the log probability of the sequence $aa$. In this case the Viterbi score would give the same result since the graph has only one path.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/unigram_aa_scored.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "For an arbitrary sequence $x$ with a graph representation $X$ and an arbitrary $n$-gram language model with graph representation $N$, the log probability of $x$ is given by:\n",
    "\n",
    "$$\n",
    "\\log p(x) = \\LSE(X \\circ N)\n",
    "$$\n",
    "\n",
    "Next, let's see how to represent a bigram language model as a graph. From there, the generalization to arbitrary order $n$ is relatively straightforward. Assume again we have the token set $\\{a, b, c\\}$. The bigram model is shown in the graph below.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/bigram.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Each state is labeled with the token representing the most recently seen input. For a bigram model we only need to remember the previous token to know which score to use on when processing the next token. For a trigram model we would need to remember the previous two tokens. For an $n$-gram model we would need to remember the previous $n-1$ tokens. The label and score pair leaving each state represent the corresponding conditional probability (technically these should be log probabilities). Each state has an outgoing arc for every possible token in the token set.\n",
    "\n",
    "---\n",
    "\n",
    "### <a name=\"ngram_example\"></a>Example\n",
    "\n",
    "Compute the number of states and arcs in a graph representation of an $n$-gram language model for a given order $n$ and a token set size of $v$.\n",
    "\n",
    "For order $n$, the graph needs a state for every possible token sequence of length $n-1$. This means that the graph will have $v^{n-1}$ states. Each state has $v$ outgoing arcs. Thus the total number of arcs in the graph is $v \\cdot v^{n-1}= v^n$. This should be expected given that the language model assigns a score for every possible sequence of length $n$.\n",
    "\n",
    "---\n",
    "\n",
    "## Automatic Segmentation Criterion\n",
    "$\\newcommand{\\mX}{{\\bf X}}\n",
    "\\newcommand{\\va}{{\\bf a}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\gA}{\\mathcal{A}}\n",
    "\\newcommand{\\gB}{\\mathcal{B}}\n",
    "\\newcommand{\\gE}{\\mathcal{E}}\n",
    "\\newcommand{\\gT}{\\mathcal{T}}\n",
    "\\newcommand{\\gY}{\\mathcal{Y}}\n",
    "\\newcommand{\\gZ}{\\mathcal{Z}}\n",
    "\\newcommand{\\sos}{\\textrm{<s>}}\n",
    "\\newcommand{\\eos}{\\textrm{</s>}}\n",
    "$\n",
    "In speech recognition and other problems, we often need to compute a conditional probability of an output sequence given an input sequence when the two sequences do not have the same length. The Automatic Segmentation criterion (ASG) is one of several common loss functions for which this is possible. However, ASG is limited to the case when the output sequence is no longer than the input sequence. \n",
    "\n",
    "Assume we have an input sequence of vectors $\\mX = [\\vx_1, \\ldots, \\vx_T]$ of length $T$ and an output token sequence $\\vy = [y_1, \\ldots, y_U]$ of length $U$ such that $U \\le T$. We don't know the actual alignment between $\\mX$ and $\\vy$ and in most applications, including speech recognition, we don't need it. To get around not knowing this alignment, the ASG criterion marginalizes over all possible alignments between $\\mX$ and $\\vy$.\n",
    "\n",
    "In ASG, the output sequence is aligned to a given input, by allowing one or more consecutive repeats of each token in the output. Let's look at an example. Suppose we have an input of length $5$ and the output sequence $ab$. Some possible alignments of the output are $aaabb$, $abbbb$, and $aaaab$. Some invalid alignmets are $abbbba$, $aaab$, and $aaaaa$. These are invalid because first corresponds to the output $aba$, the second is too short, and the third corresponds to the output $a$.\n",
    "\n",
    "For each time-step of the input, we have a model $s_t(y)$ which assigns a score for every possible output token $y$. Let $\\va = [a_1, \\ldots, a_T]$ be one possible aligment between $\\mX$ and $\\vy$. The alignment $\\va$ also has length $T$. To compute a score for $\\va$, we sum the sequence of scores for each token:\n",
    "\n",
    "$$\n",
    "s(\\va) = \\sum_{t=1}^T s_t(a_t)\n",
    "$$\n",
    "\n",
    "Let $\\gA_{\\mX,\\vy}$ denote the set of all possible alignments between $\\mX$ and $\\vy$. We can then use the individual alignment scores to compute a conditional probability of the output $\\vy$ given the input $\\mX$:\n",
    "\n",
    "$$\n",
    "\\log p(\\vy \\mid \\mX) = \\LSE_{\\va \\in \\gA_{\\mX, \\vy}} s(\\va) - \\log Z.\n",
    "$$\n",
    "\n",
    "The normalization term $Z$ is given by:\n",
    "\n",
    "$$\n",
    "Z = \\sum_{\\va \\in \\gZ_\\mX} e^{s(\\va)}\n",
    "$$\n",
    "\n",
    "where $\\gZ_\\mX$ is the set of all possible length $T$ alignments (the same length as $X$). Computing the summations over $\\gA_{\\mX,\\vy}$ and $\\gZ_\\mX$ explicitly is not tractable because the sizes of these sets grow rapidly with the lengths of $\\mX$ and $\\vy$. We will instead use automata to encode these sets and efficiently compute the summation using the forward score operation.\n",
    "\n",
    "Let's start with the normalization term $Z$. The set $\\gZ_\\mX$ just encodes all possible outputs of length $T$, where $T$ is the length of $\\mX$. As an example, assume $T=4$ and we have three possible output tokens $\\{a, b, c\\}$. If the scores for each otuput are independent, we can represent $\\gZ_\\mX$ with the graph below. The scores on the arcs are given by the model $s_t(y)$. These scores are often called the *emissions*, and the graph itself is sometimes called the emissions graph. We'll use $\\gE$ to represent the emissions graph. In this case the emissions graph $\\gE$ is the same as the normalization graph $\\gZ_\\mX$; however, in general they may be different. The normalization term is the forward score of the emissions graph, $Z = \\LSE(\\gE)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14f710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the eemisisons graph with\n",
    "# randomly sampled emissions scores, s_t(y):\n",
    "E = gtn.linear_graph(4, 3)\n",
    "E.set_weights(np.random.randn(4*3).round(decimals=1))\n",
    "gtn.draw(E, \"figures/nb/asg_emissions.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650315d7",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_emissions.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    An emissions graph $\\gE$ with $T=4$ time-steps and a token set of $\\{a, b, c\\}$.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Let's turn to the set $\\gA_{\\mX, \\vy}$ which we will also represent as an acceptor. This acceptor should have a path for every possible alignment between $\\mX$ and $\\vy$. We'll construct $\\gA_{\\mX, \\vy}$ in two steps. First, we can encode the set of allowed alignments of arbitrary length for a given sequence $\\vy$ with a graph, $\\gA_\\vy$. For example, for the sequence $ab$ the graph $\\gA_\\vy$ is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d348a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "AY = gtn.Graph()\n",
    "AY.add_node(start=True)\n",
    "AY.add_node()\n",
    "AY.add_node(accept=True)\n",
    "AY.add_arc(src_node=0, dst_node=0, label=0)\n",
    "AY.add_arc(src_node=0, dst_node=1, label=0)\n",
    "AY.add_arc(src_node=1, dst_node=1, label=1)\n",
    "AY.add_arc(src_node=1, dst_node=2, label=1)\n",
    "gtn.draw(AY, \"figures/nb/asg_alignments.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c598d17",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_alignments.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    The ASG alignment graph $\\gA_\\vy$ for the sequence $ab$. The graph encodes the fact that each output token can repeat one or more times in an arbitrary length alignment.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "This graph has a simple interpretation. Each token in the output $ab$ can repeat one or more times in the alignment. We can then construct $\\gA_{\\mX,\\vy}$ by intersecting $\\gA_\\vy$ with the emissions graph $\\gE$ which represents all possible sequences of length $T$. This gives $\\gA_{\\mX,\\vy} = \\gA_\\vy \\circ \\gA$. An example of $\\gA_{\\mX, \\vy}$ is shown below for the sequence $ab$ with $T=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac917f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "AXY = gtn.intersect(AY, E)\n",
    "gtn.draw(AXY, \"figures/nb/asg_constrained.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd29679",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_constrained.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "In terms of graph operations, we can write the ASG criterion as:\n",
    "\n",
    "$$\n",
    "p(\\vy \\mid \\mX) = \\LSE(\\gA_\\vy \\circ \\gE) - \\LSE(\\gE)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Aside: Global or Local Normalization\n",
    "\n",
    "The equation for the ASG criterion is *globally normalized*. The term $Z$ is the global normalization term and it ensure that the conditional probability $p(\\vy \\mid \\mX)$ is a valid distribution; that is it sums to one over $\\vy$. The global normalization term $Z$ (also known as the partition function) is often the most expensive part of the loss to compute.\n",
    "\n",
    "In some cases the global normalization can be avoided by using a *local normalization*. For example, in the version of ASG presented above, the path score for $\\va$ decomposes into a separate score for each time-step. In this case, we can compute the exact some loss by normalizing the scores $s_t(y)$ at each time-step and dropping the global normalizer $Z$. Concretely, we compute the normalized scores at each time-step:\n",
    "\n",
    "$$\n",
    "p_t(y) = \\frac{e^{s_t(y)}}{\\sum_z e^{s_t(z)}}.\n",
    "$$\n",
    "\n",
    "We then replace the unnormalized scores with the log-normalized scores when computing the score for an alignment:\n",
    "\n",
    "$$\n",
    "\\log p(\\va) = \\sum_{t=1}^T \\log p_t(a_t)\n",
    "$$\n",
    "\n",
    "As a last step, we remove the global normalization term $Z$ from the loss function, but leave it otherwise unchanged:\n",
    "\n",
    "$$\n",
    "\\log p(\\vy \\mid \\mX) = \\LSE_{\\va \\in \\gA_{\\mX, \\vy}} \\log p(\\va)\n",
    "$$\n",
    "\n",
    "In the version which uses graph operations, we use the log-normalized scores $\\log p_t(y)$ for the arc weights on the emissions graph $\\gE$. The graph based loss function then simplifies to: \n",
    "\n",
    "$$\n",
    "p(\\vy \\mid \\mX) = \\LSE(\\gA_\\vy \\circ \\gE)\n",
    "$$\n",
    "\n",
    "We can prove that this locally normalized version of the ASG loss is equivalent to the globally normalized version. Specifically we'd like to show that:\n",
    "\n",
    "$$\n",
    "\\LSE_{\\va \\in \\gA_{\\mX, \\vy}} \\log p(\\va) = \\LSE_{\\va \\in \\gA_{\\mX, \\vy}} s(\\va) - \\log Z\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\va) = \\sum_{t=1}^T \\frac{e^{s_t(a)}}{\\sum_z e^{s_t(z)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Transitions\n",
    "\n",
    "The original ASG loss function also includes bigram transition scores. We can include transitions scores, $h(a_{t-1}, a_t)$, in the score for an alignment:\n",
    "\n",
    "$$\n",
    "s(\\va) = \\sum_{t=1}^T s_t(a_t) + h(a_t, a_{t-1})\n",
    "$$\n",
    "\n",
    "where $a_0$ is a special start of sequence token $\\textrm{<s>}$. We can use the alignment scores in the same manner as above and the rest of the loss function remains unchanged. \n",
    "\n",
    "Let's next see how to incorporate transitions using an acceptor and graph operations. I'll rely on the ideas introduced in the section on [$n$-gram langauge models](#ngrams), so now is a good time to review that section. The first step is to encode the bigram model as a graph as shown below:\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_bigrams.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    The acceptor represents a bigram transition model for the token set $\\{a, b, c\\}$ (the arc weights are not shown). Each path begins in the start state denoted by the start-of-sequence symbol $\\sos$.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "To incorporate transition scores for the output sequence, we intersect the bigram graph $\\gB$ with the output alignment graph $\\gA_\\vy$. To incorporate transition scores in the normalization term $Z$, we intersect $\\gB$ with the emissions graph $\\gE$. The loss function using graph operations including transitions becomes:\n",
    "\n",
    "$$\n",
    "p(\\vy \\mid \\mX) = \\LSE(\\gB \\circ \\gA_\\vy \\circ \\gE) - \\LSE(\\gB \\circ \\gE)\n",
    "$$\n",
    "\n",
    "We see here an example of the expressive power of a graph-based implementation of the loss function. In a non-graph based implementation of ASG, the use of a bigram transition function is hard-coded. In the graph-based version, the transition model $\\gB$ could be a unigram, trigram, or otherwise arbitrary $n$-gram. Of course, the size of the transition graph increases rapidly with the order $n$ and the size of the token set (see the $n$-gram [example](#ngram_example)). This causes problems with both sample and computational efficiency. Thus, in practice ASG is used with a bigram transition model and token sets sizes rarely than a hundred.\n",
    "\n",
    "The arc weights on the transition graph (the scores $h(a_t, a_{t-1})$), are typically parameters of the model and learned directly. This means we need to compute the gradient of the ASG loss with respect to these scores. These derivatives are straightforward to compute in a framework with automatic differentiation.\n",
    "\n",
    "A complete implementation of the ASG loss function which takes as input an emissions graph $\\gE$, a transitions graph $\\gB$, and an output alignment graph $\\gA_\\vy$ is shown below. I'd like to make three observations about this code: \n",
    "\n",
    "1. The implementation is concise. Given the appropriate graph inputs, the complete loss function requires only eight short lines using ten function calls. \n",
    "2. The code should look familiar to users of tensor-based frameworks like PyTorch. Other than the different operation names, the imperative style and gradient computation is no different with graphs than it is with tensors. \n",
    "3. The code is generic. For example, we can use a trigram model for the input graph `B` without needing to change the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d3d5cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ASG loss is 7.013.\n"
     ]
    }
   ],
   "source": [
    "def ASG(E, B, AY):\n",
    "    # Compute constrained and normalization graphs:\n",
    "    AXY = gtn.intersect(gtn.intersect(B, AY), E)\n",
    "    ZX = gtn.intersect(B, E)\n",
    "    \n",
    "    # Forward both graphs:\n",
    "    AXY_score = gtn.forward_score(AXY)\n",
    "    ZX_score = gtn.forward_score(ZX)\n",
    "    \n",
    "    # Compute the loss:\n",
    "    loss = gtn.negate(gtn.subtract(AXY_score, ZX_score))\n",
    "    \n",
    "    # Clear the previous gradients:\n",
    "    E.zero_grad()\n",
    "    B.zero_grad()\n",
    "    \n",
    "    # Compute gradients:\n",
    "    gtn.backward(loss, retain_graph=False)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Make the bigram transition graph for the token set\n",
    "# {a, b, c} with gradient computation enabled:\n",
    "B = gtn.Graph(calc_grad=True)\n",
    "B.add_node(start=True, accept=True)\n",
    "B.add_node(accept=True)\n",
    "B.add_node(accept=True)\n",
    "B.add_node(accept=True)\n",
    "for i in range(4):\n",
    "    for j in range(3):\n",
    "        B.add_arc(src_node=i, dst_node=(j + 1), label=j)\n",
    "        \n",
    "# Call the ASG loss:\n",
    "loss = ASG(E, B, AY)\n",
    "print(f\"The ASG loss is {loss:.3f}.\")\n",
    "\n",
    "# Access the graph containing the gradient for B:\n",
    "dB = B.grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e2858",
   "metadata": {},
   "source": [
    "### ASG with Transducers\n",
    "\n",
    "As a final step, I'll show how to construct the ASG criterion from even simpler transducer buildling blocks. The advantage of this approach is that it lets us easily experiment with changes to the criterion at an even deeper level.\n",
    "\n",
    "Our goal is to compute $\\gA_\\vy$ from simpler graphs instead of hard-coding its structure directly. We'll use a ta token graph\n",
    "- TODO explain how we will compute A from the tokens and the label graph\n",
    "\n",
    "As a first step, we encode the assumption that each token in an output maps to one or more repeats of the same token in an alignment. For example for the output $ab$ and alignment $aaaabb$ the token $a$ maps to $aaaa$ and $b$ maps to  $bb$. For the token $a$, we can construct the token graph $\\gT_a$ shown below which has the desired property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fea93594",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ta = gtn.Graph()\n",
    "Ta.add_node(start=True)\n",
    "Ta.add_node(accept=True)\n",
    "Ta.add_arc(src_node=0, dst_node=1, label=0)\n",
    "Ta.add_arc(src_node=1, dst_node=1, ilabel=0, olabel=gtn.epsilon)\n",
    "\n",
    "gtn.draw(Ta, \"figures/nb/asg_tokens_a.svg\", isymbols=symbols, osymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18575255",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_tokens_a.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Since an output sequence can consist of any sequence of tokens from the token set, we construct a complete token graph $\\gT$ by taking the union of the individual token graphs and computing the closure. If we have a token set $\\{a, b, c\\}$, then we construct individual token graphs $\\gT_a$, $\\gT_b$, and $\\gT_c$. The complete token graph $\\gT$ is given by $\\gT = (\\gT_a + \\gT_b + \\gT_c)^*$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "293071be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the individual token graphs\n",
    "Tinds = []\n",
    "for i in range(3):\n",
    "    Tind = gtn.Graph()\n",
    "    Tind.add_node(start=True)\n",
    "    Tind.add_node(accept=True)\n",
    "    Tind.add_arc(src_node=0, dst_node=1, label=i)\n",
    "    Tind.add_arc(src_node=1, dst_node=1, ilabel=i, olabel=gtn.epsilon)\n",
    "    Tinds.append(Tind)\n",
    "\n",
    "T = gtn.closure(gtn.union(Tinds))\n",
    "gtn.draw(T, \"figures/nb/asg_tokens.svg\", isymbols=symbols, osymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd216a8",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_tokens.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Next steps\n",
    "- construct $\\gY$.\n",
    "- construct $\\gA_Y$ from $\\gT$ and $\\gY$\n",
    "- from here everything is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d665a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
