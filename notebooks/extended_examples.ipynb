{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99201bae",
   "metadata": {},
   "source": [
    "# Extended Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1fbd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "             .container { width:900px !important; }\n",
       "             .figure {display:table; margin:auto;}\n",
       "             .img {float: left; margin: 10px;}\n",
       "             .caption {display:table-caption;caption-side:bottom;text-align:justify;color: rgba(0,0,0,0.6);}\n",
       "             td { background: white;}\n",
       "            .table_caption {\n",
       "                font-size:14px;\n",
       "                float: left;\n",
       "                text-align: justify;\n",
       "                color: rgba(0,0,0,0.6);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gtn\n",
    "import nb_utils\n",
    "import math\n",
    "nb_utils.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c2cca",
   "metadata": {},
   "source": [
    "## Counting $n$-grams\n",
    "\n",
    "In this example, we'll use graph operations to count the number of $n$-grams in a string. \n",
    "\n",
    "Suppose we have a string $aaabaa$ and we want to know the frequency of each bigram. In this case the bigrams contained in the string are $aa$, $ab$, and $ba$ with frequencies of $3$, $1$, and $1$ respectively. In the general case we are given an input string $x$ and an $n$-gram, $y$, and the goal is to count the number of occurrences of $y$ in $x$.\n",
    "\n",
    "For a given $n$-gram, the first step is to construct the graph which matches that $n$-gram at any location in the string. We want to construct the graph equivalent of the regular expression $.*y.*$ where $.*$ indicates zero of more occurrences of any token in the token set. \n",
    "\n",
    "Suppose we want to count the number of ocurrences of the bigram $aa$ in $aaabaa$. For the bigram $aa$, and the token set alphabet $\\{a, b, c\\}$, the $n$-gram matching graph is shown below.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/bigram_aa.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "      A hierarchy of classes graph.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "We can encode the string $aaabaa$ as a simple chain graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab21d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = {0: \"a\", 1: \"b\", 2: \"c\"}\n",
    "\n",
    "# Encode the string \"aaabaa\" as integer ids:\n",
    "x = [0, 0, 0, 1, 0, 0]  \n",
    "g = gtn.Graph()\n",
    "g.add_node(start=True)\n",
    "for i, l in enumerate(x):\n",
    "    g.add_node(accept=(i + 1==len(x)))\n",
    "    g.add_arc(src_node=i, dst_node=i + 1, label=l)\n",
    "gtn.draw(g, \"figures/nb/ngram_string.svg\", isymbols=symbols)\n",
    "\n",
    "bigram = gtn.Graph()\n",
    "bigram.add_node(start=True)\n",
    "bigram.add_node()\n",
    "bigram.add_node(accept=True)\n",
    "bigram.add_arc(src_node=0, dst_node=1, label=0)\n",
    "bigram.add_arc(src_node=1, dst_node=2, label=0)\n",
    "for l in range(len(symbols)):\n",
    "    bigram.add_arc(src_node=0, dst_node=0, label=l)\n",
    "    bigram.add_arc(src_node=2, dst_node=2, label=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228718b",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/ngram_string.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "      A hierarchy of classes graph.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "We then compute the intersection of the graph representing the string and the graph representing the bigram. The number of paths in this graph represents the number of occurrences of the bigram in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b849b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_paths = gtn.intersect(g, bigram)\n",
    "gtn.draw(bigram_paths, \"figures/nb/bigram_paths.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b245ed2",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/bigram_paths.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "      A hierarchy of classes graph.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Since each path has a weight of $0$, we can count the number of unique paths in the intersected graph by using the forward score. Assume the intersected graph has $p$ paths. The forward score of the graph is $s = \\log \\sum_{i=1}^p e^0 = \\log p$. So the total number of paths is $p = e^s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f37a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of occurrences of 'ab' in 'aaabaa' is 3\n"
     ]
    }
   ],
   "source": [
    "s = gtn.forward_score(bigram_paths)\n",
    "p = math.exp(s.item())\n",
    "print(f\"The number of occurrences of 'ab' in 'aaabaa' is {p:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb468b5",
   "metadata": {},
   "source": [
    "## Edit Distance\n",
    "\n",
    "In this example we'll use transducers to compute the Levenshtein edit distance between two sequences. The edit distance is a way to measure the similarity between to sequences by computing the minimum number of operations required to change one sequence into the other. The Levenshtein edit distance allows for insertion, deletion, and substitution operations. \n",
    "\n",
    "For example, consider the two strings \"saturday\" and \"sunday\". The edit distance between them is $3$. One way to minimally edit \"saturday\" to \"sunday\" is with two deletions (D) and a substitution (S) as below:\n",
    "```\n",
    "s a t u r d a y\n",
    "  D D   S\n",
    "s     u n d a y\n",
    "```\n",
    "\n",
    "We can compute the edit distance between two strings with the use transducers. The idea is to transduce the first string into the second according to the allowed operations encoded as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd4d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = gtn.Graph()\n",
    "edits.add_node(True)\n",
    "edits.add_node(accept=True)\n",
    "edits.add_node(accept=True)\n",
    "edits.add_node(accept=True)\n",
    "edits.add_node(accept=True)\n",
    "edits.add_arc(0, 1, gtn.epsilon, 0, 1)\n",
    "edits.add_arc(0, 1, gtn.epsilon, 1, 1)\n",
    "edits.add_arc(0, 2, 0, gtn.epsilon, 1)\n",
    "edits.add_arc(0, 2, 1, gtn.epsilon, 1)\n",
    "edits.add_arc(0, 3, 0, 1, 1)\n",
    "edits.add_arc(0, 3, 1, 0, 1)\n",
    "edits.add_arc(0, 4, 0, 0)\n",
    "edits.add_arc(0, 4, 1, 1)\n",
    "gtn.draw(edits, \"figures/nb/edits.svg\", isymbols=symbols, osymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c652c",
   "metadata": {},
   "source": [
    "We first construct an edits graph $E$ which encodes the allowed operations. An example of an edits graph assuming a token set of $\\{a, b\\}$ is shown below. The insertion of a token is represented by the arcs from state $0$ to state $1$ hand has a cost of $1$. The deletion of a token is represented by the arcs from state $0$ to state $2$ which also incur a cost of $1$. All possible substitutions are encoded in the arcs from $0$ to $3$ and again have a cost of $1$. We also have to encode the possibility of leaving a token unchanged. This is represented on the arcs from $0$ to $4$, and the cost is $0$.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/edits.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "      TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "We then take closure of the edits graph $E$ to represent the fact that we can make zero or more of any of the allowed edits. We then encode the first string $x$ in a graph $X$ and the second string $y$ in a graph $Y$. All possible ways of editing $x$ to $y$ can be computed by taking the composition:\n",
    "\n",
    "$$\n",
    "P = X \\circ E^* \\circ Y\n",
    "$$\n",
    "\n",
    "The graph $P$ represents the set of all possible unique ways we can edit the string $x$ into $y$. The score of a given path in $P$ is the associated cost. We can then find the edit distance by computing the path with the smallest score in $P$. For this, we use Viterbi algorithm with a $\\min$ instead of a $\\max$. Alternatively, we can use weights of $-1$ instead of $1$ in $E$ and use the Viterbi algorithm unchanged. The actual edits (*i.e.* the insertions, deletions, and substitutions) can be found by computing the Viterbi path.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Compute the edit distance between $x = abab$ and $y = aaabb$ using graphs.\n",
    "\n",
    "We first construct the edits graph $E^*$ and the graphs $X$ and $Y$ corresponding to the strings $x$ and $y$. Note that we are using a minimal representation of $E^*$, but the graph is equivalent to the closure of the edits graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a65fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = gtn.Graph()\n",
    "edits.add_node(start=True, accept=True)\n",
    "edits.add_arc(0, 0, gtn.epsilon, 0, -1)\n",
    "edits.add_arc(0, 0, gtn.epsilon, 1, -1)\n",
    "edits.add_arc(0, 0, 0, gtn.epsilon, -1)\n",
    "edits.add_arc(0, 0, 1, gtn.epsilon, -1)\n",
    "edits.add_arc(0, 0, 0, 1, -1)\n",
    "edits.add_arc(0, 0, 1, 0, -1)\n",
    "edits.add_arc(0, 0, 0, 0)\n",
    "edits.add_arc(0, 0, 1, 1)\n",
    "\n",
    "X = gtn.Graph()\n",
    "X.add_node(start=True)\n",
    "X.add_node()\n",
    "X.add_node()\n",
    "X.add_node()\n",
    "X.add_node(accept=True)\n",
    "X.add_arc(0, 1, 0)\n",
    "X.add_arc(1, 2, 1)\n",
    "X.add_arc(2, 3, 0)\n",
    "X.add_arc(3, 4, 1)\n",
    "\n",
    "Y = gtn.Graph()\n",
    "Y.add_node(start=True)\n",
    "Y.add_node()\n",
    "Y.add_node()\n",
    "Y.add_node()\n",
    "Y.add_node()\n",
    "Y.add_node(accept=True)\n",
    "Y.add_arc(0, 1, 0)\n",
    "Y.add_arc(1, 2, 0)\n",
    "Y.add_arc(2, 3, 0)\n",
    "Y.add_arc(3, 4, 1)\n",
    "Y.add_arc(4, 5, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320c201",
   "metadata": {},
   "source": [
    "Note also that with the edits graph, $E^*$ we use scores of $-1$ so that we can use the Viterbi score which is the maximum scoring path to find the smallest edit distance. The next step is to compute $P = X \\circ E^* \\circ Y$.  The graph $P$ is shown below. Each path in $P$ represents a unique conversion of $X$ into $Y$ using insertion, deletion, and substitution operations. The negation of the score of the path is the number of such operations required.\n",
    "\n",
    "For example the path along the state sequence $0 \\rightarrow 1 \\rightarrow 4 \\rightarrow 9 \\rightarrow 16 \\rightarrow 25$ converts $x$ to $y$ with a substitution for the second letter and an insertion at the end:\n",
    "```\n",
    "x = a b a b\n",
    "y = a a a b b\n",
    "      S     I\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5636a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_paths = gtn.compose(X, gtn.compose(edits, Y))\n",
    "gtn.draw(edit_paths, \"figures/nb/edit_paths.svg\", isymbols=symbols, osymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12173dbf",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/edit_paths.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129e0d4",
   "metadata": {},
   "source": [
    "The Viterbi score and Viterbi path then yield the edit distance between $x$ and $y$ and one sequence of edit operations required to attain the edit distance. The Viterbi path for the example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9f2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The edit distance is the negation of the Viterbi score\n",
    "edit_distance = -gtn.viterbi_score(edit_paths).item()\n",
    "edit_path = gtn.viterbi_path(edit_paths)\n",
    "gtn.draw(edit_path, \"figures/nb/edit_path.svg\", isymbols=symbols, osymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e88d15",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/edit_path.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## $n$-gram Language Model\n",
    "$\\DeclareMathOperator*{\\LSE}{\\textrm{LSE}}$In this example we'll encode an $n$-gram language model as an acceptor. We'll then use the acceptor to compute the language model probability of a given sequence.\n",
    "\n",
    "Let's start with a very simple example. Suppose we have the token set $\\{a, b, c, \\textrm{</s>}\\}$ and we want to construct a unigram langauge model. Note  $\\textrm{</s>}$ is the end of sentence token and is required for the language model to be a valid probability distribution. Given counts of occurrences for each token in the vocabulary, we can construct an acceptor to represent the unigram language model. Suppose we are given the log probabilities $0.5$, $0.2$, and $0.3$ for $a$, $b$, and $c$ respectively. The corresponding unigram graph is shown below. Note the edge weights are the log probabilities.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/unigram.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Now assume we are given the sequence $aa$ for which we would like to compute the probability. The probability under the language model is $\\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$. We can compute the log probability of $aa$ by intersecting its graph representation $X$ with the unigram graph $U$ and then computing the forward score:\n",
    "\n",
    "$$\n",
    "\\log p(aa) = \\LSE (X \\circ U)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2742490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unigram graph U:\n",
    "U = gtn.Graph()\n",
    "U.add_node(start=True, accept=True)\n",
    "#U.add_node(accept=True)\n",
    "U.add_arc(src_node=0, dst_node=0, label=0)\n",
    "U.add_arc(src_node=0, dst_node=0, label=1)\n",
    "U.add_arc(src_node=0, dst_node=0, label=2)\n",
    "U.set_weights([math.log(p) for p in [0.5, 0.2, 0.3]])\n",
    "\n",
    "# The graph representing the sequence \"aa\":\n",
    "X = gtn.Graph()\n",
    "X.add_node(start=True)\n",
    "X.add_node()\n",
    "X.add_node(accept=True)\n",
    "X.add_arc(src_node=0, dst_node=1, label=0)\n",
    "X.add_arc(src_node=1, dst_node=2, label=0)\n",
    "\n",
    "# Compute the unigram score of X:\n",
    "x_scored = gtn.intersect(X, U)\n",
    "x_prob = math.exp(gtn.forward_score(x_scored).item())\n",
    "gtn.draw(x_scored, \"figures/nb/unigram_aa_scored.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482fdfc",
   "metadata": {},
   "source": [
    "The graph below shows the intersection $X \\circ U$. The arc edges in the intersected graph contain the correct unigram scores, and the forward score gives the log probability of the sequence $aa$. In this case the Viterbi score would give the same result since the graph has only one path.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/unigram_aa_scored.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "For an arbitrary sequence $x$ with a graph representation $X$ and an arbitrary $n$-gram language model with graph representation $N$, the log probability of $x$ is given by:\n",
    "\n",
    "$$\n",
    "\\log p(x) = \\LSE(X \\circ N)\n",
    "$$\n",
    "\n",
    "Next, let's see how to represent a bigram language model as a graph. From there, the generalization to arbitrary order $n$ is relatively straightforward. Assume again we have the token set $\\{a, b, c\\}$. The bigram model is shown in the graph below.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/bigram.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Each state is labeled with the token representing the most recently seen input. For a bigram model we only need to remember the previous token to know which score to use on when processing the next token. For a trigram model we would need to remember the previous two tokens. For an $n$-gram model we would need to remember the previous $n-1$ tokens. The label and score pair leaving each state represent the corresponding conditional probability (technically these should be log probabilities). Each state has an outgoing arc for every possible token in the token set.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Compute the number of states and arcs in a graph representation of an $n$-gram language model for a given order $n$ and a token set size of $v$.\n",
    "\n",
    "For order $n$, the graph needs a state for every possible token sequence of length $n-1$. This means that the graph will have $v^{n-1}$ states. Each state has $v$ outgoing arcs. Thus the total number of arcs in the graph is $v \\cdot v^{n-1}= v^n$. This should be expected given that the language model assigns a score for every possible sequence of length $n$.\n",
    "\n",
    "---\n",
    "\n",
    "## Automatic Segmentation Criterion\n",
    "$\\newcommand{\\bX}{{\\bf X}}\n",
    "\\newcommand{\\by}{{\\bf y}}\n",
    "\\newcommand{\\bx}{{\\bf x}}\n",
    "\\newcommand{\\ba}{{\\bf a}}\n",
    "\\newcommand{\\bp}{{\\bf p}}\n",
    "$\n",
    "In speech recognition and other problems, we often need to compute a conditional probability of an output sequence given an input sequence when the two sequences do not have the same length. The Automatic Segmentation criterion (ASG) is one of several common loss functions for which this is possible. However, ASG is limited to the case when the output sequence is no longer than the input sequence. \n",
    "\n",
    "Assume we have an input sequence of vectors $\\bX = [\\bx_1, \\ldots, \\bx_T]$ of length $T$ and an output token sequence $\\by = [y_1, \\ldots, y_U]$ of length $U$ such that $U \\le T$. We don't know the actual alignment between $\\by$ and $\\bX$ and in most applications, including speech recognition, we don't need it. To get around not knowing this alignment, the ASG criterion marginalizing over all possible alignments between $\\bX$ and $\\by$.\n",
    "\n",
    "In ASG, the output sequence is aligned to a given input, by allowing one or more consecutive repeats of each token in the output. Let's look at an example. Suppose we have an input of length $5$ and the output sequence $ab$. Some possible alignments of the output are $aaabb$, $abbbb$, and $aaaab$. Some invalid alignmets are $abbbba$, $aaab$, and $aaaaa$. The first corresponds to the output $aba$, the second is too short, and the third corresponds to the output $a$.\n",
    "\n",
    "For each time-step of the input, our model assigns a score for every possible output token. Let $\\ba = [a_1, \\ldots, a_T]$ be one possible aligment between $\\bX$ and $\\by$. The alignment $\\ba$ also has length $T$. To compute a score for $\\ba$ sum the sequence of scores for each token:\n",
    "\n",
    "$$\n",
    "s(\\ba) = \\sum_{t=1}^T s_t(a_t)\n",
    "$$\n",
    "\n",
    "Let $\\mathcal{A}_{\\bX,\\by}$ denote the set of all possible alignments between $\\bX$ and $\\by$. We can then use the individual alignment scores to compute a conditional probability of the output $\\by$ given the input $\\bX$:\n",
    "\n",
    "$$\n",
    "\\log p(\\by \\mid \\bX) = \\sum_{\\ba \\in \\mathcal{A}_{\\bX, \\by}} e^{s(\\ba)} - \\log Z\n",
    "$$\n",
    "\n",
    "where $Z$ is a normalization term:\n",
    "\n",
    "$$\n",
    "Z = \\sum_{\\bp \\in \\mathcal{Z}_\\bX} e^{s(\\bp)}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{A}_\\bX$ is the set of all possible alignments of the same length as $X$. Computing the summations over $\\mathcal{A}_{\\bX,\\by}$ and $\\mathcal{Z}_\\bX$ explicitly is not tractable because the sizes of these sets grow rapidly with the lengths of $\\bX$ and $\\by$. We will instead use automata to encode these sets and efficiently compute the summation using the forward score algorithm.\n",
    "\n",
    "Let's start with the normalization term $Z$. The set $\\mathcal{Z}_\\bX$ just encodes all possible outputs of length $T$, where $T$ is the length of $\\bX$. Assuming $T=4$ and we have three possible output tokens. If the scores for each otuput are independent, we can represent $\\mathcal{Z}_\\bX$ with the graph below. Each output token at each step is given a score by the model. These scores are often called the *emissions* and the graph itself is sometimes called the emissions graph. We'll use $\\mathcal{E}$ to represent the emissions graph. In this case the emissions graph $\\mathcal{E}$ is the same as the normalizaation graph $\\mathcal{Z}_\\bX$; however, in general they may be different. The normalization term is the forward score of the emissions graph, $Z = \\LSE(\\mathcal{E})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14f710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = gtn.linear_graph(4, 3)\n",
    "gtn.draw(E, \"figures/nb/asg_emissions.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650315d7",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_emissions.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b41568",
   "metadata": {},
   "source": [
    "Let's turn to the set $\\mathcal{A}_{\\bX, \\by}$ which we will also represent as an acceptor. This acceptor should have a path for every possible alignment between $\\bX$ and $\\by$. We'll construct $\\mathcal{A}_{\\bX, \\by}$ in two steps. First, we can encode the set of allowed alignment of arbitrary length for a given sequence $\\by$ using a graph. For example, for the sequence $ab$ the graph is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d348a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ay = gtn.Graph()\n",
    "ay.add_node(start=True)\n",
    "ay.add_node()\n",
    "ay.add_node(accept=True)\n",
    "ay.add_arc(src_node=0, dst_node=0, label=0)\n",
    "ay.add_arc(src_node=0, dst_node=1, label=0)\n",
    "ay.add_arc(src_node=1, dst_node=1, label=1)\n",
    "ay.add_arc(src_node=1, dst_node=2, label=1)\n",
    "gtn.draw(ay, \"figures/nb/asg_alignments.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c598d17",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_alignments.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "This graph has a simple interpretation. Each token in the output $ab$ can repeat one or more times in the alignment. We'll use the symbol $\\mathcal{A}\\by$ to represent this graph as it is only dependent on $\\by$ and not $\\bX$. We can then construct $\\mathcal{A}_{\\bX,\\by}$ by intersecting $\\mathcal{A}_\\by$ with the graph containing all possible sequences of length $T$. The latter graph is precisely the emissions graph $\\mathcal{E}$ we constructed above, so the $\\mathcal{A}_{\\bX,\\by} = \\mathcal{A}_\\by \\circ \\mathcal{E}$. An example graph is shown below for the sequence $ab$ with $T=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac917f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "axy = gtn.intersect(ay, E)\n",
    "gtn.draw(axy, \"figures/nb/asg_constrained.svg\", isymbols=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd29679",
   "metadata": {},
   "source": [
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_constrained.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "In terms of graph operations, we can then write the ASG loss function as:\n",
    "\n",
    "$$\n",
    "p(\\by \\mid \\bX) = \\LSE(\\mathcal{A}_{\\by} \\circ \\mathcal{E}) - \\LSE(\\mathcal{E})\n",
    "$$\n",
    "\n",
    "### Transitions\n",
    "\n",
    "The original ASG loss function also includes bigram transition scores. If we let $h(a_{t-1}, a_t)$ denote the transition function, then we can incorporate the transition score into the score of the alignment:\n",
    "\n",
    "$$\n",
    "s(\\ba) = \\sum_{t=1}^T s_t(a_t) + h(a_t, a_{t-1})\n",
    "$$\n",
    "\n",
    "where we assume $a_0$ is a special start of sequence token $\\textrm{<s>}$. We can use the alignment scores in the same manner as above and the rest of the loss function remains unchanged. \n",
    "\n",
    "We'll next show how to incorporate transitions using an acceptor and graph operations. I'll rely on the ideas introduced in the section on $n$-gram langauge models, so now is a good time to read or review that section. The first step is to encode the bigram model as a graph as shown below:\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/nb/asg_bigrams.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    TODO\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "### ASG with Transducers\n",
    "\n",
    "As a final step, I'll show how to construct the ASG criterion from even simpler transducer buildling blocks. The advantage of this approach is that it lets us experiment with changes to the criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4861013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
