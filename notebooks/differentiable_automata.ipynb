{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dd4834",
   "metadata": {},
   "source": [
    "# Differentiable Automata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2d5fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "             .container { width:900px !important; }\n",
       "             .figure {display:table; margin:auto;}\n",
       "             .img {float: left; margin: 10px;}\n",
       "             .caption {display:table-caption;caption-side:bottom;text-align:justify;color: rgba(0,0,0,0.6);}\n",
       "             td { background: white;}\n",
       "            .table_caption {\n",
       "                font-size:14px;\n",
       "                float: left;\n",
       "                text-align: justify;\n",
       "                color: rgba(0,0,0,0.6);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gtn\n",
    "import nb_utils\n",
    "nb_utils.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcb2be",
   "metadata": {},
   "source": [
    "Many operations used with vectors, matrices, and tensors are differentiable. What that means is we can compute the change in any of the output elements with respect to an infinitesimal change in any of the input elements. For example, consider a vector $\\mathbf{z} = f(\\mathbf{x}, \\mathbf{y})$ which is the output of a function of two vectors $\\mathbf{x}$ and $\\mathbf{y}$. The Jacobian of $\\mathbf{z}$ with respect to $\\mathbf{x}$ is the  natrix of partial derivatives with entries $\\frac{\\partial \\mathbf{z}_i}{\\partial \\mathbf{x}_j}$. The gradient is defined as the tensor of partial derivatives of a scalar function. So if $f(\\mathbf{x}) \\in \\mathbb{R}$ is a scalar function, then the gradient is \n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\left[ \\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}_{1}}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}_{n}}  \\right].\n",
    "$$\n",
    "\n",
    "In the same way, we can compute partial derivatives of the arc weights of an output graph for a given operation with respect to the arc weights of any of the input graphs. Take the union operation as an example. Suppose we are given two graphs, $A$ and $B$ and we construct the concatenated graph $C = AB$ as in the figure below.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/concat_grad_A.svg\" style=\"width:150px;\"/>\n",
    "  </div>\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/concat_grad_B.svg\" style=\"width:150px;\"/> \n",
    "  </div>\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/concat_grad_C.svg\" style=\"width:340px;\"/> \n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "     The union of the graphs $A$ (left) and $B$ (middle) produce $C$ (right). For each graph the arc weights are shown as variables on the edges.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "For each of the arc weights $C_i$ in the union graph $C$, we can compute the partial derivative with respect to the input arc weights. For any arc in $C$, it has either a corresponding arc in $A$ or $B$ from which it gets its weight or it has a weight of zero. The partial derivative of an output arc weight $C_i$ with respect to an input arc weight $A_j$ or $B_j$ is $1$ if the two arcs correspond and $0$ otherwise. For example, in the above graphs we have,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_1}{\\partial A_1} = 1, \\quad \\frac{\\partial C_2}{\\partial A_2} = 1, \\quad \\frac{\\partial C_4}{\\partial B_1} = 1, \\quad\\textrm{and}\\quad \\frac{\\partial C_5}{\\partial B_2} = 1.\n",
    "$$\n",
    "\n",
    "The remaining partial derivatives are all $0$. For example,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_1}{\\partial A_2} = 0, \\quad \\frac{\\partial C_1}{\\partial B_1} = 0, \\quad \\textrm{and} \\quad \\frac{\\partial C_1}{\\partial B_2} = 0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Compute the partial derivatives of the arcs of the closure of the graph $A$ with respect to the input arcs.\n",
    "\n",
    "<div class=\"figure\">\n",
    "  <div class=\"img\">\n",
    "    <img src=\"figures/closure_grad.svg\"/>\n",
    "  </div>\n",
    "  <div class=\"caption\" markdown=\"span\">\n",
    "    The closure of the graph $A$ is given above. The weights are denoted by the variables $w_i$.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "The non-zero partial derivatives are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial w_2}{\\partial A_1} = 0 \\quad \\textrm{and} \\quad \\frac{\\partial w_3}{\\partial A_2} = 0.\n",
    "$$\n",
    "\n",
    "The remaining partial derivatives are zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial w_1}{\\partial A_1} = 0, \\quad \\frac{\\partial w_1}{\\partial A_2} = 0, \\quad \\frac{\\partial w_2}{\\partial A_2} = 0, \\quad \\frac{\\partial w_3}{\\partial A_1} = 0, \\quad \\frac{\\partial w_4}{\\partial A_1} = 0, \\quad \\textrm{and} \\quad \\frac{\\partial w_4}{\\partial A_2} = 0.\n",
    "$$\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
